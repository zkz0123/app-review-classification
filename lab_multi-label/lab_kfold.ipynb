{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "889a6b9e-8e08-44fd-8257-6fce40caf525",
   "metadata": {},
   "source": [
    "# **Classification of app reviews for requirements engineering using deep learning models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383bb831-c726-4152-981c-60f88ccfea66",
   "metadata": {},
   "source": [
    "## **Data loading and processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457b24a0-89b7-400f-9ebe-ec274a9b74d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load multi-label dataset from CSV file\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset('csv', data_files='dataset/gpt_multi_label_100.csv')\n",
    "\n",
    "print(dataset)\n",
    "print(dataset['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e36617-2bd8-4c78-823b-67cf1abf80aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert labels into a list\n",
    "def process_labels(example):\n",
    "    example['label'] = [example['feature request'],\n",
    "                        example['bug report'],\n",
    "                        example['rating'],\n",
    "                        example['user experience']]\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(process_labels)\n",
    "\n",
    "print(dataset['train'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957ccc8e-a117-492c-9e28-42fba8d3ac74",
   "metadata": {},
   "source": [
    "*K-Fold Cross-Validation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071236f8-2769-4fc8-9d53-7d5a75579fae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import pandas as pd\n",
    "# Convert data to a Pandas DataFrame for processing\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "# Split the dataset into a train+validation(90%) and test(10%)\n",
    "train_val_df, test_df = train_test_split(df, test_size=0.1)\n",
    "# Convert to Hugging Face Dataset\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "test_dataset = test_dataset.remove_columns(['__index_level_0__'])\n",
    "\n",
    "# Perform K-Fold cross-validation on the train+validation set\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# Save data for each fold\n",
    "folds = []\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(train_val_df)):\n",
    "    # Split into training and validation sets for the current fold\n",
    "    train_data = train_val_df.iloc[train_index]\n",
    "    val_data = train_val_df.iloc[val_index]\n",
    "    \n",
    "    # Convert to Hugging Face Dataset\n",
    "    train_dataset = Dataset.from_pandas(train_data)\n",
    "    val_dataset = Dataset.from_pandas(val_data)\n",
    "    train_dataset = train_dataset.remove_columns(['__index_level_0__'])\n",
    "    val_dataset = val_dataset.remove_columns(['__index_level_0__'])\n",
    "    \n",
    "    # Save the datasets for the current fold\n",
    "    folds.append({\n",
    "        'train': train_dataset,\n",
    "        'validation': val_dataset\n",
    "    })\n",
    "\n",
    "\n",
    "# Print the structure of the first fold's datasets\n",
    "print(\"Fold 1 Training Dataset:\")\n",
    "print(folds[0]['train'])\n",
    "print(\"Fold 1 Validation Dataset:\")\n",
    "print(folds[0]['validation'])\n",
    "print(\"Test Dataset:\")\n",
    "print(test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6156a40-f912-41f7-a0d4-f8c331618c44",
   "metadata": {},
   "source": [
    "## **BERT Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0554f00-90a7-4e3b-8bf3-020848491efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Data preprocessing function\n",
    "def preprocess_data(example):\n",
    "    tokenized = tokenizer(example['review'], \n",
    "                          padding='max_length', \n",
    "                          truncation=True, \n",
    "                          max_length=128)\n",
    "    tokenized['labels'] = example['label']  # Add multi-labels\n",
    "    return tokenized\n",
    "\n",
    "# Training parameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "patience = 3\n",
    "\n",
    "# Directory to save the models\n",
    "save_dir = os.path.join(\".\", \"models\", \"kf_bert\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "save_path = os.path.join(save_dir, f\"best_model_{timestamp}.pth\")\n",
    "backup_path = os.path.join(save_dir, f\"backup_model_{timestamp}.pth\")\n",
    "best_model_paths = []\n",
    "\n",
    "# Define a function to save the model\n",
    "def save_model(model, path):\n",
    "    try:\n",
    "        torch.save(model.state_dict(), path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save the model to {path}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Store the validation losses for each fold\n",
    "all_val_losses = []\n",
    "\n",
    "# Loop through each fold\n",
    "for fold_id, fold_data in enumerate(folds):\n",
    "    print(f\"\\nTraining Fold {fold_id + 1}:\")\n",
    "    \n",
    "    # Get training and validation datasets for the current fold\n",
    "    train_dataset = fold_data['train'].map(preprocess_data, batched=True)\n",
    "    valid_dataset = fold_data['validation'].map(preprocess_data, batched=True)\n",
    "    test_dataset = test_dataset.map(preprocess_data, batched=True)\n",
    "    \n",
    "    # Convert datasets to PyTorch format\n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    valid_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    # Data loaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Load BERT model\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n",
    "    model.classifier = nn.Linear(768, 4)  # Replace classifier head for multi-label task\n",
    "    model.to(device)\n",
    "    \n",
    "    # Optimizer and learning rate scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=1e-5)\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    warmup_steps = int(0.1 * total_steps)\n",
    "    warmup_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
    "    )\n",
    "    reduce_scheduler = ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Early stopping and best model saving\n",
    "    val_losses = []\n",
    "    global_step = 0\n",
    "    epochs_no_improve = 0\n",
    "    best_val_loss = float('inf')\n",
    "    save_path = os.path.join(save_dir, f\"fold{fold_id + 1}_best_model_{timestamp}.pth\")\n",
    "    best_model_paths.append(save_path)\n",
    "    \n",
    "    # Training and validation loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            # Prepare data\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels.float())\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update learning rate with warm-up scheduler\n",
    "            if global_step < warmup_steps:\n",
    "                warmup_scheduler.step()\n",
    "            global_step += 1\n",
    "        \n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_dataloader:\n",
    "                inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(**inputs)\n",
    "                loss = loss_fn(outputs.logits, labels.float())\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.sigmoid(outputs.logits).cpu().numpy() > 0.5\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds)\n",
    "        \n",
    "        # Calculate average validation loss\n",
    "        avg_val_loss = val_loss / len(valid_dataloader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Adjust learning rate using reduce-on-plateau scheduler\n",
    "        if global_step >= warmup_steps:\n",
    "            reduce_scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Save the best model based on validation loss\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Save the best model\n",
    "            saved = save_model(model, save_path)\n",
    "            if saved:\n",
    "                best_model_path = save_path\n",
    "            else:\n",
    "                # Attempt backup path if saving to primary path fails\n",
    "                backup_saved = save_model(model, backup_path)\n",
    "                if backup_saved:\n",
    "                    best_model_path = backup_path\n",
    "                else:\n",
    "                    print(\"Failed to save the model to both primary and backup paths. Stopping training.\")\n",
    "                    sys.exit(1)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping due to no improvement in validation loss.\")\n",
    "                break\n",
    "                \n",
    "    # Save the validation losses for this fold            \n",
    "    all_val_losses.append(val_losses)\n",
    "\n",
    "print(\"Training completed for all folds!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6e4b69-0618-4b2f-b579-0d118800492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the validation loss curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "for fold_id, val_loss in enumerate(all_val_losses):\n",
    "    plt.plot(val_loss, label=f'Fold {fold_id + 1}')\n",
    "plt.title('Validation Loss Curve for Each Fold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c87a4b-0d68-4f7b-819d-93fbdf1d24f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "# Full training + fine-tuning code\n",
    "print(\"\\nRetraining the best model using the full training + validation set...\")\n",
    "\n",
    "# Get the index and path of the model with the lowest validation loss\n",
    "best_model_index = np.argmin([min(val_loss) for val_loss in all_val_losses])\n",
    "best_model_path = best_model_paths[best_model_index]\n",
    "print(f\"Path of the model with the lowest validation loss: {best_model_path}\")\n",
    "\n",
    "# Load the best model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n",
    "model.classifier = nn.Linear(768, 4)  # Replace the classification head\n",
    "model.load_state_dict(torch.load(best_model_path, weights_only=True))\n",
    "model.to(device)\n",
    "\n",
    "# Prepare the full training + validation set\n",
    "full_train_dataset = Dataset.from_pandas(train_val_df)\n",
    "full_train_dataset = full_train_dataset.map(preprocess_data, batched=True)\n",
    "full_train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# DataLoader\n",
    "full_train_dataloader = DataLoader(full_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=1e-5)\n",
    "total_steps = len(full_train_dataloader) * (epochs // 2)  # Reduce epochs for fine-tuning\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "model.train()\n",
    "for epoch in range(epochs // 2):  # Use 50% of the total epochs for fine-tuning\n",
    "    total_loss = 0\n",
    "    for batch in full_train_dataloader:\n",
    "        # Prepare data\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        loss = loss_fn(outputs.logits, labels.float())\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Print training loss for each epoch\n",
    "    avg_train_loss = total_loss / len(full_train_dataloader)\n",
    "    print(f\"Fine-tuning Epoch {epoch + 1}/{epochs // 2} - Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "# Save the final fine-tuned model\n",
    "final_model_path = Path(save_dir) / \"final_model_fine_tuned_bert.pth\"\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"The final fine-tuned model has been saved to: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc76794-947a-4d40-989f-0db56d018442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load(final_model_path, weights_only=True))  \n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Evaluate on the test set\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        preds = torch.sigmoid(outputs.logits).cpu().numpy() > 0.5\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds)\n",
    "\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=['feature request', 'bug report', 'rating', 'user experience'], zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17371cd1-9bf8-455f-b72c-d4855f043170",
   "metadata": {},
   "source": [
    "## **RoBERTa Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7139acf-48d2-4a32-a298-720f6f8b9b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "patience = 3\n",
    "\n",
    "# Create a directory to save the model\n",
    "save_dir = os.path.join(\".\", \"models\", \"kf_roberta\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "backup_path = os.path.join(save_dir, f\"backup_model_{timestamp}.pth\")\n",
    "\n",
    "# Load the Roberta tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Data preprocessing function\n",
    "def preprocess_data(example):\n",
    "    tokenized = tokenizer(example['review'],\n",
    "                          padding='max_length',\n",
    "                          truncation=True,\n",
    "                          max_length=128)\n",
    "    tokenized['labels'] = example['label']\n",
    "    return tokenized\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "best_model_paths = []\n",
    "all_val_losses = []\n",
    "\n",
    "def save_model(model, path):\n",
    "    try:\n",
    "        torch.save(model.state_dict(), path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save the model to {path}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Train and validate for each fold\n",
    "for fold_id, fold_data in enumerate(folds):\n",
    "    print(f\"\\nTraining Fold {fold_id + 1}:\")\n",
    "\n",
    "    # Preprocess the training and validation data for the current fold\n",
    "    train_dataset = fold_data['train'].map(preprocess_data, batched=True)\n",
    "    valid_dataset = fold_data['validation'].map(preprocess_data, batched=True)\n",
    "    test_dataset = test_dataset.map(preprocess_data, batched=True)\n",
    "\n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    valid_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Load the Roberta model\n",
    "    model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=4)\n",
    "    # RobertaForSequenceClassification already has a classifier layer; num_labels=4 can be used directly.\n",
    "    # If customization is needed, replace as follows:\n",
    "    model.classifier.out_proj = nn.Linear(model.classifier.out_proj.in_features, 4)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=1e-5)\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    warmup_steps = int(0.1 * total_steps)\n",
    "    warmup_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
    "    )\n",
    "    reduce_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "    val_losses = []\n",
    "    global_step = 0\n",
    "    epochs_no_improve = 0\n",
    "    best_val_loss = float('inf')\n",
    "    fold_best_path = os.path.join(save_dir, f\"fold{fold_id + 1}_best_model_{timestamp}.pth\")\n",
    "    best_model_paths.append(fold_best_path)\n",
    "\n",
    "    # Start the training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels.float())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if global_step < warmup_steps:\n",
    "                warmup_scheduler.step()\n",
    "            global_step += 1\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_dataloader:\n",
    "                inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(**inputs)\n",
    "                loss = loss_fn(outputs.logits, labels.float())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = (torch.sigmoid(outputs.logits).cpu().numpy() > 0.5).astype(int)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds)\n",
    "\n",
    "        avg_val_loss = val_loss / len(valid_dataloader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # Use ReduceLROnPlateau only after warm-up\n",
    "        if global_step >= warmup_steps:\n",
    "            reduce_scheduler.step(avg_val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Save the best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            saved = save_model(model, fold_best_path)\n",
    "            if not saved:\n",
    "                backup_saved = save_model(model, backup_path)\n",
    "                if not backup_saved:\n",
    "                    print(\"Failed to save model to both paths. Exiting.\")\n",
    "                    import sys\n",
    "                    sys.exit(1)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping due to no improvement in validation loss.\")\n",
    "                break\n",
    "\n",
    "    all_val_losses.append(val_losses)\n",
    "\n",
    "print(\"Training completed for all folds!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de583520-455b-4dc4-9ca2-d74441305d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the validation loss curve for each fold\n",
    "plt.figure(figsize=(6,4))\n",
    "for fold_id, val_loss in enumerate(all_val_losses):\n",
    "    plt.plot(val_loss, label=f'Fold {fold_id + 1}')\n",
    "plt.title('Validation Loss Curve for Each Fold (Roberta)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3a6b18-73ab-4564-9e18-00ea42960c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model with the best validation loss across all folds\n",
    "print(\"\\nRetraining the best model using the full training + validation set...\")\n",
    "best_model_index = np.argmin([min(val_loss) for val_loss in all_val_losses])\n",
    "best_model_path = best_model_paths[best_model_index]\n",
    "print(f\"Path of the model with the lowest validation loss: {best_model_path}\")\n",
    "\n",
    "# Reload the best model\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=4)\n",
    "model.classifier.out_proj = nn.Linear(model.classifier.out_proj.in_features, 4)\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.to(device)\n",
    "\n",
    "# Fine-tune using the full training + validation set\n",
    "full_train_dataset = Dataset.from_pandas(train_val_df)\n",
    "full_train_dataset = full_train_dataset.map(preprocess_data, batched=True)\n",
    "full_train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "full_train_dataloader = DataLoader(full_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=1e-5)\n",
    "total_steps = len(full_train_dataloader) * (epochs // 2)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs // 2):\n",
    "    total_loss = 0\n",
    "    for batch in full_train_dataloader:\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        loss = loss_fn(outputs.logits, labels.float())\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(full_train_dataloader)\n",
    "    print(f\"Fine-tuning Epoch {epoch + 1}/{epochs // 2} - Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "final_model_path = Path(save_dir) / \"final_model_fine_tuned_roberta.pth\"\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"The final fine-tuned model has been saved to: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41aff2-d04b-403f-8e0b-58693b85729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the final model and evaluate on the test set\n",
    "model.load_state_dict(torch.load(final_model_path))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        preds = (torch.sigmoid(outputs.logits).cpu().numpy() > 0.5).astype(int)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds)\n",
    "\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds,\n",
    "                            target_names=['feature request', 'bug report', 'rating', 'user experience'],\n",
    "                            zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0cf9e5-514e-47fe-864d-608e447ba900",
   "metadata": {},
   "source": [
    "## **BART Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69feda7d-d245-47cf-9387-b555245172f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForSequenceClassification\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "patience = 3\n",
    "\n",
    "# Create a directory to save the model\n",
    "save_dir = os.path.join(\".\", \"models\", \"kf_bart\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "backup_path = os.path.join(save_dir, f\"backup_model_{timestamp}.pth\")\n",
    "\n",
    "# Load the BART tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "\n",
    "# Data preprocessing function\n",
    "def preprocess_data(example):\n",
    "    tokenized = tokenizer(example['review'],\n",
    "                          padding='max_length',\n",
    "                          truncation=True,\n",
    "                          max_length=128)\n",
    "    tokenized['labels'] = example['label']\n",
    "    return tokenized\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "best_model_paths = []\n",
    "all_val_losses = []\n",
    "\n",
    "def save_model(model, path):\n",
    "    try:\n",
    "        torch.save(model.state_dict(), path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save the model to {path}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Train and validate for each fold\n",
    "for fold_id, fold_data in enumerate(folds):\n",
    "    print(f\"\\nTraining Fold {fold_id + 1}:\")\n",
    "\n",
    "    # Preprocess the training and validation data for the current fold\n",
    "    train_dataset = fold_data['train'].map(preprocess_data, batched=True)\n",
    "    valid_dataset = fold_data['validation'].map(preprocess_data, batched=True)\n",
    "    # Re-process the test dataset in each fold to ensure correctness (slightly reduces performance)\n",
    "    test_dataset_mapped = test_dataset.map(preprocess_data, batched=True)\n",
    "\n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    valid_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    test_dataset_mapped.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset_mapped, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Load the BART model\n",
    "    model = BartForSequenceClassification.from_pretrained('facebook/bart-base', num_labels=4)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=1e-5)\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    warmup_steps = int(0.1 * total_steps)\n",
    "    warmup_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
    "    )\n",
    "    reduce_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "    val_losses = []\n",
    "    global_step = 0\n",
    "    epochs_no_improve = 0\n",
    "    best_val_loss = float('inf')\n",
    "    fold_best_path = os.path.join(save_dir, f\"fold{fold_id + 1}_best_model_{timestamp}.pth\")\n",
    "    best_model_paths.append(fold_best_path)\n",
    "\n",
    "    # Start the training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels.float())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if global_step < warmup_steps:\n",
    "                warmup_scheduler.step()\n",
    "            global_step += 1\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_dataloader:\n",
    "                inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(**inputs)\n",
    "                loss = loss_fn(outputs.logits, labels.float())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = (torch.sigmoid(outputs.logits).cpu().numpy() > 0.5).astype(int)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds)\n",
    "\n",
    "        avg_val_loss = val_loss / len(valid_dataloader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        if global_step >= warmup_steps:\n",
    "            reduce_scheduler.step(avg_val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Save the best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            saved = save_model(model, fold_best_path)\n",
    "            if not saved:\n",
    "                backup_saved = save_model(model, backup_path)\n",
    "                if not backup_saved:\n",
    "                    print(\"Failed to save model to both paths. Exiting.\")\n",
    "                    import sys\n",
    "                    sys.exit(1)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping due to no improvement in validation loss.\")\n",
    "                break\n",
    "\n",
    "    all_val_losses.append(val_losses)\n",
    "\n",
    "print(\"Training completed for all folds!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b56dc1-3309-415c-a483-f1a3beb94e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the validation loss curve for each fold\n",
    "plt.figure(figsize=(6, 4))\n",
    "for fold_id, val_loss in enumerate(all_val_losses):\n",
    "    plt.plot(val_loss, label=f'Fold {fold_id + 1}')\n",
    "plt.title('Validation Loss Curve for Each Fold (BART)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72fb21d-8ca2-4055-9d4a-c370100fb5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model with the best validation loss across all folds\n",
    "print(\"\\nRetraining the best model using the full training + validation set...\")\n",
    "best_model_index = np.argmin([min(val_loss) for val_loss in all_val_losses])\n",
    "best_model_path = best_model_paths[best_model_index]\n",
    "print(f\"Path of the model with the lowest validation loss: {best_model_path}\")\n",
    "\n",
    "# Reload the best model\n",
    "model = BartForSequenceClassification.from_pretrained('facebook/bart-base', num_labels=4)\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.to(device)\n",
    "\n",
    "# Fine-tune using the full training + validation set\n",
    "full_train_dataset = Dataset.from_pandas(train_val_df)\n",
    "full_train_dataset = full_train_dataset.map(preprocess_data, batched=True)\n",
    "full_train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "full_train_dataloader = DataLoader(full_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=1e-5)\n",
    "total_steps = len(full_train_dataloader) * (epochs // 2)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs // 2):\n",
    "    total_loss = 0\n",
    "    for batch in full_train_dataloader:\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        loss = loss_fn(outputs.logits, labels.float())\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(full_train_dataloader)\n",
    "    print(f\"Fine-tuning Epoch {epoch + 1}/{epochs // 2} - Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "final_model_path = Path(save_dir) / \"final_model_fine_tuned_bart.pth\"\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"The final fine-tuned model has been saved to: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ee01a8-a91a-4bd7-a8d7-b2d01f4e196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the final model for test set evaluation\n",
    "model.load_state_dict(torch.load(final_model_path))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Evaluate on the test set\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "# Use the preprocessed test_dataset_mapped (re-process if not done previously)\n",
    "test_dataset_mapped = test_dataset.map(preprocess_data, batched=True)\n",
    "test_dataset_mapped.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataloader = DataLoader(test_dataset_mapped, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        preds = (torch.sigmoid(outputs.logits).cpu().numpy() > 0.5).astype(int)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds)\n",
    "\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds,\n",
    "                            target_names=['feature request', 'bug report', 'rating', 'user experience'],\n",
    "                            zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
